# config/training/vit.yaml
optimizer:
  name: "adamw"  # or "sgd"
  lr: 1e-4
  weight_decay: 1e-3
  # SGD specific
  momentum: 0.9
  nesterov: false
  # AdamW specific
  betas: [0.9, 0.999]
  eps: 1e-8

scheduler:
  name: "cosine"  # Cosine is typically better for transformers
  eta_min: 1e-6
  pct_start: 0.3
  div_factor: 25.0
  final_div_factor: 1e4

batch_size: 128  # Smaller batch size due to ViT memory requirements
epochs: 20  # Fewer epochs due to ViT convergence speed
label_smoothing: 0.1
gradient_clip: 1.0
seed: 42
num_workers: 8
lr_tta: True
mixed_precision: True